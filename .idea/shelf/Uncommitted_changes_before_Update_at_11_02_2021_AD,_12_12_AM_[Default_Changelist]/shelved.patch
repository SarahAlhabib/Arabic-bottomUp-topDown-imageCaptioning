Index: arabic_dataset.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding: utf-8 -*-\n\"\"\"Arabic_Dataset.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1svBkj0-_o783sEozr_5cRnXJyl6MMQ3I\n\"\"\"\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n!unzip /content/drive/MyDrive/Dataset.zip\n\n!pip install pyarabic\n\nimport pyarabic.araby as araby\nimport pyarabic.number as number\n\nimport re\nimport string\nimport pyarabic.araby as araby\nfrom pyarabic.araby import strip_tatweel, strip_tashkeel, tokenize, is_arabicrange\n\nclass Arabic_preprocessing:\n    \n    \n    def __init__(self):\n        \n        #preparing punctuations list\n        arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n        english_punctuations = string.punctuation\n        self.all_punctuations = set(arabic_punctuations + english_punctuations)\n\n        \n    def normalize_arabic(self, text):\n        text = re.sub(\"[إأآاٱ]\", \"ا\", text)\n        text = re.sub(\"ى\", \"ي\", text)\n        text = re.sub(\"ة\", \"ه\", text)  # replace ta2 marboota by ha2\n        text = re.sub(\"گ\", \"ك\", text)\n        text = strip_tatweel(text) #remove tatweel \n        text = strip_tashkeel(text) #remove tashkeel\n        text = re.sub(r'\\bال(\\w\\w+)', r'\\1', text)  # remove al ta3reef\n\n        return text\n\n\n    def remove_punctuations(self, text):\n        return ''.join([c for c in text if c not in self.all_punctuations]) #remove punctuations\n\n\n    def remove_repeating_char(self, text):\n        return re.sub(r'(.)\\1+', r'\\1', text)\n\n\n    def remove_english_characters(self, text):\n        return re.sub(r'[a-zA-Z]+', '', text)\n  \n    def preprocess_arabic_text(self, text):\n        text = self.remove_punctuations(text)\n        text = self.normalize_arabic(text)\n        text = self.remove_english_characters(text)\n        text = self.remove_repeating_char(text)\n        text = ' '.join([w for w in text.split() if len(w)>1 and w.isalpha()]) #remove one-character & numeric words\n        return text\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nfrom string import punctuation\nimport re\nimport nltk\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torchvision \nfrom torchnlp.encoders.text import StaticTokenizerEncoder, stack_and_pad_tensors, pad_tensor\nimport keras\n\n\ndef load_data(filename):\n  file = open(filename,'r')\n  text = file.read()\n  file.close()\n  return text\n\ndef get_captions(file_text):\n    \"\"\"given file content, returns images names and their captions as dictionary\"\"\"\n    cpts = {}\n    #loop through lines\n    for line in file_text.split('\\n'): # each line contains image name & its caption separated by tab\n        #split by tabs\n        img_cpt = line.split('\\t')\n        if len(img_cpt) < 2: continue\n        img, cpt = img_cpt\n        #remove image extension & index (remove everything befor the dot)\n        img_name = img.split('.')[0]\n        #add to dictionary\n        if img_name not in cpts:\n            cpts[img_name] = [cpt]\n        else:\n            cpts[img_name].append(cpt)\n    return cpts\n\ndef preprocess_captions(cpts):\n    \"\"\" clean captions to reduce vocabulary size we need to work with:\n        - lowercase\n        - remove punctuations\n        - remove one-character words\n        - remove words with numbers\n    \"\"\"\n    for img, cpt in cpts.items():\n        cpts_ = []\n        for c in cpt:\n            c = c.lower() #lower case all caption\n            c = ''.join([char for char in c if char not in punctuation]) #remove punctuations\n            c = ' '.join([w for w in c.split() if len(w)>1 and w.isalpha()]) #remove one-character & numeric words\n            cpts_.append(c)\n        cpts[img] = cpts_\n\n\ndef preprocess_captions(cpts):\n    \"\"\" clean captions to get rid of useless textual info & reduce vocabulary size. Preprocessing includes:\n        - remove punctuations & diacritics\n        - normalize (or standarize) Hamza & Ha2\n        - remove repeating characters\n        - remove english characters\n        - remove one-character words\n    \"\"\"\n    process_arab = Arabic_preprocessing()\n    for img, cpt in cpts.items():\n        processed_captions = [process_arab.preprocess_arabic_text(c) for c in cpt]\n        cpts[img] = processed_captions\n\ndef add_start_end_to_captions(cpts):\n    \"\"\"precede each caption with <START> and end each caption with <END>\"\"\"\n    start, end = '<START>', '<END>'\n    #start, end = 'start', 'end'\n    for k, v in cpts.items():\n        image_captions = [start + ' ' + cpt + ' ' + end for cpt in v]\n        cpts[k] = image_captions\n\n    \n    \nfilename = \"/content/Dataset/data/Flickr8k_text/Flickr8k.arabic.full.txt\"\n\n\ncaptions_file_text = load_data(filename)\n\ncaptions = get_captions(captions_file_text)\nprint('Captions #:', len(captions))\nprint('Caption example:', list(captions.values())[0])\n\n\nk = '299178969_5ca1de8e40' #2660480624_45f88b3022\nprint('before >>', captions[k])\npreprocess_captions(captions)\nprint('captions preprocessed :)')\nprint('after >>', captions[k])\n\n\n# يوضح أن (preprocess_captions(cpts)) تشتغل بشكل صحيح \nre.search('\\w+', ' 456')\ni = 0\nfor k,v in captions.items():\n    print(v)\n    i += 1\n    if i == 10: break\n\n        \nadd_start_end_to_captions(captions)\n\nfor k,v in captions.items():\n    for cpt in v:\n        a = [w for w in cpt.split() if len(w)==1 and w!='و']\n        if len(a)>0: print(cpt)\n            \n            \n  def get_vocabulary(cpts):\n    \"\"\"retruns a list of all unique words in captions\"\"\"\n    captions_flattened = [cpt for image_captions in cpts.values() for cpt in image_captions]\n    all_captions = ' '.join(captions_flattened)\n    v = set(all_captions.split())\n    return sorted(list(v))\n\nvocabulary = get_vocabulary(captions)\nprint('Vocabulary size (number of unique words):', len(vocabulary))\n\ndef get_frequent_vocabulary(cpts, frequency=5):\n    \"\"\"retruns a list of all unique words that appeared more than `frequency` times\"\"\"\n    captions_flattened = [cpt for image_captions in cpts.values() for cpt in image_captions]\n    all_captions = ' '.join(captions_flattened)\n    frequent_vocabulary = []\n    for i,v in enumerate(vocabulary):\n        if all_captions.count(v) >= frequency: frequent_vocabulary.append(v)\n    return frequent_vocabulary\n\nfrequent_vocabulary = get_frequent_vocabulary(captions, 3)\nprint('Frequent vocabulary size (number of unique words):', len(frequent_vocabulary))\n\n\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)\n\n            \nnum_words = len(frequent_vocabulary) + 1\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(captions[k])\n\n\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\n# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(captions[k])\n\n# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n\n# Calculates the max_length, which is used to store the attention weights\nmax_length = calc_max_length(train_seqs)\n\n\nprint(captions[k])\n\nprint(train_seqs)\nprint(cap_vector)\nprint(max_length)\n\n\n\n\n\n        \n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/arabic_dataset.py b/arabic_dataset.py
--- a/arabic_dataset.py	(revision cb34b603e3df8b293118ffc56ebba245ac350129)
+++ b/arabic_dataset.py	(date 1612903768624)
@@ -6,13 +6,16 @@
 Original file is located at
     https://colab.research.google.com/drive/1svBkj0-_o783sEozr_5cRnXJyl6MMQ3I
 """
+import pip
+import tensorflow as tensorflow
 
-from google.colab import drive
+"""from google.colab import drive
 drive.mount('/content/drive')
-!unzip /content/drive/MyDrive/Dataset.zip
+!unzip /content/drive/MyDrive/Dataset.zip"""
 
-!pip install pyarabic
 
+
+import pyarabic as pyarabic
 import pyarabic.araby as araby
 import pyarabic.number as number
 
@@ -72,8 +75,8 @@
 import numpy as np
 import torch
 from torch import nn
-import torchvision 
-from torchnlp.encoders.text import StaticTokenizerEncoder, stack_and_pad_tensors, pad_tensor
+import torchvision
+#from torchnlp.encoders.text import StaticTokenizerEncoder, stack_and_pad_tensors, pad_tensor
 import keras
 
 
@@ -141,7 +144,7 @@
 
     
     
-filename = "/content/Dataset/data/Flickr8k_text/Flickr8k.arabic.full.txt"
+filename = "/Users/RazanAlkhadhiri/Desktop/Arabic-Image-Captioning-master/data copy/Flickr8k_text/Flickr8k.arabic.full.txt"
 
 
 captions_file_text = load_data(filename)
@@ -175,7 +178,7 @@
         if len(a)>0: print(cpt)
             
             
-  def get_vocabulary(cpts):
+def get_vocabulary(cpts):
     """retruns a list of all unique words in captions"""
     captions_flattened = [cpt for image_captions in cpts.values() for cpt in image_captions]
     all_captions = ' '.join(captions_flattened)
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Python 3.8 (Arabic-bottomUp-topDown)\" project-jdk-type=\"Python SDK\" />\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
--- a/.idea/misc.xml	(revision cb34b603e3df8b293118ffc56ebba245ac350129)
+++ b/.idea/misc.xml	(date 1612900478433)
@@ -1,4 +1,4 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8 (Arabic-bottomUp-topDown)" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
Index: .idea/Arabic-bottomUp-topDown-imageCaptioning.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<module type=\"PYTHON_MODULE\" version=\"4\">\n  <component name=\"NewModuleRootManager\">\n    <content url=\"file://$MODULE_DIR$\" />\n    <orderEntry type=\"jdk\" jdkName=\"Python 3.8 (Arabic-bottomUp-topDown)\" jdkType=\"Python SDK\" />\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\n  </component>\n  <component name=\"TestRunnerService\">\n    <option name=\"PROJECT_TEST_RUNNER\" value=\"Unittests\" />\n  </component>\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/Arabic-bottomUp-topDown-imageCaptioning.iml b/.idea/Arabic-bottomUp-topDown-imageCaptioning.iml
--- a/.idea/Arabic-bottomUp-topDown-imageCaptioning.iml	(revision cb34b603e3df8b293118ffc56ebba245ac350129)
+++ b/.idea/Arabic-bottomUp-topDown-imageCaptioning.iml	(date 1612900478425)
@@ -2,10 +2,7 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$" />
-    <orderEntry type="jdk" jdkName="Python 3.8 (Arabic-bottomUp-topDown)" jdkType="Python SDK" />
+    <orderEntry type="jdk" jdkName="Python 3.8" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
-  <component name="TestRunnerService">
-    <option name="PROJECT_TEST_RUNNER" value="Unittests" />
-  </component>
 </module>
\ No newline at end of file
