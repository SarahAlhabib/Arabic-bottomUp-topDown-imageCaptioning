# -*- coding: utf-8 -*-
"""Arabic_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1svBkj0-_o783sEozr_5cRnXJyl6MMQ3I
"""

from google.colab import drive
drive.mount('/content/drive')
!unzip /content/drive/MyDrive/Dataset.zip

!pip install pyarabic

import pyarabic.araby as araby
import pyarabic.number as number

import re
import string
import pyarabic.araby as araby
from pyarabic.araby import strip_tatweel, strip_tashkeel, tokenize, is_arabicrange

class Arabic_preprocessing:
    
    
    def __init__(self):
        
        #preparing punctuations list
        arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''
        english_punctuations = string.punctuation
        self.all_punctuations = set(arabic_punctuations + english_punctuations)

        
    def normalize_arabic(self, text):
        text = re.sub("[إأآاٱ]", "ا", text)
        text = re.sub("ى", "ي", text)
        text = re.sub("ة", "ه", text)  # replace ta2 marboota by ha2
        text = re.sub("گ", "ك", text)
        text = strip_tatweel(text) #remove tatweel 
        text = strip_tashkeel(text) #remove tashkeel
        text = re.sub(r'\bال(\w\w+)', r'\1', text)  # remove al ta3reef
        text = re.sub(r'\sو(\w+)', r' و \1', text)   # to add space after 'و'

        return text


    def remove_punctuations(self, text):
        return ''.join([c for c in text if c not in self.all_punctuations]) #remove punctuations


    def remove_repeating_char(self, text):
        return re.sub(r'(.)\1+', r'\1', text)


    def remove_english_characters(self, text):
        return re.sub(r'[a-zA-Z]+', '', text)
  
    def preprocess_arabic_text(self, text):
        text = self.remove_punctuations(text)
        text = self.normalize_arabic(text)
        text = self.remove_english_characters(text)
        text = self.remove_repeating_char(text)
        text = ' '.join([w for w in text.split() if len(w)>1 and w.isalpha()]) #remove one-character & numeric words
        return text

import pandas as pd
import matplotlib.pyplot as plt
import string
from string import punctuation
import re
import pickle

import numpy as np
from tqdm import tqdm_notebook, tnrange
#import keras
#from keras_tqdm import TQDMNotebookCallback
# for arabic text with matplotlib
#from bidi import algorithm as bidialg
#import arabic_reshaper


def load_data(filename):
  file = open(filename,'r')
  text = file.read()
  file.close()
  return text

def get_captions(file_text):
    """given file content, returns images names and their captions as dictionary"""
    cpts = {}
    #loop through lines
    for line in file_text.split('\n'): # each line contains image name & its caption separated by tab
        #split by tabs
        img_cpt = line.split('\t')
        if len(img_cpt) < 2: continue
        img, cpt = img_cpt
        #remove image extension & index (remove everything befor the dot)
        img_name = img.split('.')[0]
        #add to dictionary
        if img_name not in cpts:
            cpts[img_name] = [cpt]
        else:
            cpts[img_name].append(cpt)
    return cpts

def preprocess_captions(cpts):
    """ clean captions to reduce vocabulary size we need to work with:
        - lowercase
        - remove punctuations
        - remove one-character words
        - remove words with numbers
    """
    for img, cpt in cpts.items():
        cpts_ = []
        for c in cpt:
            c = c.lower() #lower case all caption
            c = ''.join([char for char in c if char not in punctuation]) #remove punctuations
            c = ' '.join([w for w in c.split() if len(w)>1 and w.isalpha()]) #remove one-character & numeric words
            cpts_.append(c)
        cpts[img] = cpts_


def preprocess_captions(cpts):
    """ clean captions to get rid of useless textual info & reduce vocabulary size. Preprocessing includes:
        - remove punctuations & diacritics
        - normalize (or standarize) Hamza & Ha2
        - remove repeating characters
        - remove english characters
        - remove one-character words
    """
    process_arab = Arabic_preprocessing()
    for img, cpt in cpts.items():
        processed_captions = [process_arab.preprocess_arabic_text(c) for c in cpt]
        cpts[img] = processed_captions

def add_start_end_to_captions(cpts):
    """precede each caption with <START> and end each caption with <END>"""
    start, end = '<START>', '<END>'
    #start, end = 'start', 'end'
    for k, v in cpts.items():
        image_captions = [start + ' ' + cpt + ' ' + end for cpt in v]
        cpts[k] = image_captions

        
   #convert from text to numbers
def tokenizeText(text, cutoff_for_rare_words = 1):
    """Function to convert text to numbers. Text must be tokenzied so that
    test is presented as a list of words. The index number for a word
    is based on its frequency (words occuring more often have a lower index).
    If a word does not occur as many times as cutoff_for_rare_words,
    then it is given a word index of zero. All rare words will be zero.
    """
    
    # Flatten list if sublists are present
    if len(text) > 1:
        flat_text = [item for sublist in text for item in sublist]
    else:
        flat_text = text
    
    # get word freuqncy
    fdist = nltk.FreqDist(flat_text)

    # Convert to Pandas dataframe
    df_fdist = pd.DataFrame.from_dict(fdist, orient='index')
    df_fdist.columns = ['Frequency']

    # Sort by word frequency
    df_fdist.sort_values(by=['Frequency'], ascending=False, inplace=True)

    # Add word index
    number_of_words = df_fdist.shape[0]
    df_fdist['word_index'] = list(np.arange(number_of_words)+1)

    # replace rare words with index zero
    frequency = df_fdist['Frequency'].values
    word_index = df_fdist['word_index'].values
    mask = frequency <= cutoff_for_rare_words
    word_index[mask] = 0
    df_fdist['word_index'] =  word_index
    
    # Convert pandas to dictionary
    word_dict = df_fdist['word_index'].to_dict()
    
    # Use dictionary to convert words in text to numbers
    text_numbers = []
    for string in text:
        string_numbers = [word_dict[word] for word in string]
        text_numbers.append(string_numbers)  
    
    return (text_numbers)


def save_desc(description, filename):
  lines = list()
  for K,desc_list in description.item():
    for desc in  desc_lis :
      lines.append(k+''+desc)
  data =''.join(lines)
  file=open(filename,'w')
  file.write(data)
  file.close()

    
    
    
    
filename = "/content/Dataset/data/Flickr8k_text/Flickr8k.arabic.full.txt"
captions_file_text = load_data(filename)
captions = get_captions(captions_file_text)
print('Captions #:', len(captions))
print('Caption example:', list(captions.values())[0])

k = '299178969_5ca1de8e40' #2660480624_45f88b3022
print('before >>', captions[k])
preprocess_captions(captions)
print('captions preprocessed :)')
print('after >>', captions[k])




# يوضح أن (preprocess_captions(cpts)) تشتغل بشكل صحيح 
re.search('\w+', ' 456')
i = 0
for k,v in captions.items():
    print(v)
    i += 1
    if i == 10: break

        
        
        
        
def tokenizeText2(text):
  text = nltk.word_tokenize(text)
  return text

text = "السلام عليكم ورحمة الله"
print(tokenizeText(text))






# An example tokenised list
print(captions[k])
text_numbers = tokenizeText(captions[k])
print (text_numbers)



