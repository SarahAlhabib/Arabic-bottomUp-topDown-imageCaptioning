# -*- coding: utf-8 -*-
"""Arabic_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1svBkj0-_o783sEozr_5cRnXJyl6MMQ3I
"""

from google.colab import drive
drive.mount('/content/drive')
!unzip /content/drive/MyDrive/Dataset.zip

!pip install pyarabic

import pyarabic.araby as araby
import pyarabic.number as number

import re
import string
import pyarabic.araby as araby
from pyarabic.araby import strip_tatweel, strip_tashkeel, tokenize, is_arabicrange

class Arabic_preprocessing:
    
    
    def __init__(self):
        
        #preparing punctuations list
        arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''
        english_punctuations = string.punctuation
        self.all_punctuations = set(arabic_punctuations + english_punctuations)

        
    def normalize_arabic(self, text):
        text = re.sub("[إأآاٱ]", "ا", text)
        text = re.sub("ى", "ي", text)
        text = re.sub("ة", "ه", text)  # replace ta2 marboota by ha2
        text = re.sub("گ", "ك", text)
        text = strip_tatweel(text) #remove tatweel 
        text = strip_tashkeel(text) #remove tashkeel
        text = re.sub(r'\bال(\w\w+)', r'\1', text)  # remove al ta3reef

        return text


    def remove_punctuations(self, text):
        return ''.join([c for c in text if c not in self.all_punctuations]) #remove punctuations


    def remove_repeating_char(self, text):
        return re.sub(r'(.)\1+', r'\1', text)


    def remove_english_characters(self, text):
        return re.sub(r'[a-zA-Z]+', '', text)
  
    def preprocess_arabic_text(self, text):
        text = self.remove_punctuations(text)
        text = self.normalize_arabic(text)
        text = self.remove_english_characters(text)
        text = self.remove_repeating_char(text)
        text = ' '.join([w for w in text.split() if len(w)>1 and w.isalpha()]) #remove one-character & numeric words
        return text

import pandas as pd
import matplotlib.pyplot as plt
import string
from string import punctuation
import re
import nltk
import numpy as np
import torch
from torch import nn
import torchvision 
from torchnlp.encoders.text import StaticTokenizerEncoder, stack_and_pad_tensors, pad_tensor
import keras


def load_data(filename):
  file = open(filename,'r')
  text = file.read()
  file.close()
  return text

def get_captions(file_text):
    """given file content, returns images names and their captions as dictionary"""
    cpts = {}
    #loop through lines
    for line in file_text.split('\n'): # each line contains image name & its caption separated by tab
        #split by tabs
        img_cpt = line.split('\t')
        if len(img_cpt) < 2: continue
        img, cpt = img_cpt
        #remove image extension & index (remove everything befor the dot)
        img_name = img.split('.')[0]
        #add to dictionary
        if img_name not in cpts:
            cpts[img_name] = [cpt]
        else:
            cpts[img_name].append(cpt)
    return cpts

def preprocess_captions(cpts):
    """ clean captions to reduce vocabulary size we need to work with:
        - lowercase
        - remove punctuations
        - remove one-character words
        - remove words with numbers
    """
    for img, cpt in cpts.items():
        cpts_ = []
        for c in cpt:
            c = c.lower() #lower case all caption
            c = ''.join([char for char in c if char not in punctuation]) #remove punctuations
            c = ' '.join([w for w in c.split() if len(w)>1 and w.isalpha()]) #remove one-character & numeric words
            cpts_.append(c)
        cpts[img] = cpts_


def preprocess_captions(cpts):
    """ clean captions to get rid of useless textual info & reduce vocabulary size. Preprocessing includes:
        - remove punctuations & diacritics
        - normalize (or standarize) Hamza & Ha2
        - remove repeating characters
        - remove english characters
        - remove one-character words
    """
    process_arab = Arabic_preprocessing()
    for img, cpt in cpts.items():
        processed_captions = [process_arab.preprocess_arabic_text(c) for c in cpt]
        cpts[img] = processed_captions

def add_start_end_to_captions(cpts):
    """precede each caption with <START> and end each caption with <END>"""
    start, end = '<START>', '<END>'
    #start, end = 'start', 'end'
    for k, v in cpts.items():
        image_captions = [start + ' ' + cpt + ' ' + end for cpt in v]
        cpts[k] = image_captions

    
    
filename = "/content/Dataset/data/Flickr8k_text/Flickr8k.arabic.full.txt"


captions_file_text = load_data(filename)

captions = get_captions(captions_file_text)
print('Captions #:', len(captions))
print('Caption example:', list(captions.values())[0])


k = '299178969_5ca1de8e40' #2660480624_45f88b3022
print('before >>', captions[k])
preprocess_captions(captions)
print('captions preprocessed :)')
print('after >>', captions[k])


# يوضح أن (preprocess_captions(cpts)) تشتغل بشكل صحيح 
re.search('\w+', ' 456')
i = 0
for k,v in captions.items():
    print(v)
    i += 1
    if i == 10: break

        
add_start_end_to_captions(captions)

for k,v in captions.items():
    for cpt in v:
        a = [w for w in cpt.split() if len(w)==1 and w!='و']
        if len(a)>0: print(cpt)
            
            
  def get_vocabulary(cpts):
    """retruns a list of all unique words in captions"""
    captions_flattened = [cpt for image_captions in cpts.values() for cpt in image_captions]
    all_captions = ' '.join(captions_flattened)
    v = set(all_captions.split())
    return sorted(list(v))

vocabulary = get_vocabulary(captions)
print('Vocabulary size (number of unique words):', len(vocabulary))

def get_frequent_vocabulary(cpts, frequency=5):
    """retruns a list of all unique words that appeared more than `frequency` times"""
    captions_flattened = [cpt for image_captions in cpts.values() for cpt in image_captions]
    all_captions = ' '.join(captions_flattened)
    frequent_vocabulary = []
    for i,v in enumerate(vocabulary):
        if all_captions.count(v) >= frequency: frequent_vocabulary.append(v)
    return frequent_vocabulary

frequent_vocabulary = get_frequent_vocabulary(captions, 3)
print('Frequent vocabulary size (number of unique words):', len(frequent_vocabulary))


def calc_max_length(tensor):
    return max(len(t) for t in tensor)

            
num_words = len(frequent_vocabulary) + 1
tokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words,
                                                  oov_token="<unk>",
                                                  filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
tokenizer.fit_on_texts(captions[k])


tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'

# Create the tokenized vectors
train_seqs = tokenizer.texts_to_sequences(captions[k])

# Pad each vector to the max_length of the captions
# If you do not provide a max_length value, pad_sequences calculates it automatically
cap_vector = keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')

# Calculates the max_length, which is used to store the attention weights
max_length = calc_max_length(train_seqs)


print(captions[k])

print(train_seqs)
print(cap_vector)
print(max_length)





        

